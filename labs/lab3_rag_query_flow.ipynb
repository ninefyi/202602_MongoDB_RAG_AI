{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building the RAG Query Flow\n",
    "\n",
    "This lab walks you through the core steps of a Retrieval Augmented Generation (RAG) query. We'll use Voyage-AI to embed a user's question, then query MongoDB Atlas Vector Search to retrieve relevant context, and finally, structure an augmented prompt for a Large Language Model (LLM).\n",
    "\n",
    "## Objectives\n",
    "- Embed a user's query using Voyage-AI.\n",
    "- Perform a vector search in MongoDB Atlas to retrieve relevant documents.\n",
    "- Combine the retrieved documents into a context for the LLM.\n",
    "- Construct an augmented prompt for an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Complete Lab 1 and Lab 2 (MongoDB Atlas setup and data ingestion).\n",
    "- Ensure your MongoDB Atlas Vector Search index is built and ready.\n",
    "- Python environment set up with `pymongo`, `voyageai`, and `python-dotenv` installed.\n",
    "- `.env` file containing `MONGO_URI` and `VOYAGE_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Variables and Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import voyageai\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Voyage-AI Client\n",
    "voyage_api_key = os.environ.get(\"VOYAGE_API_KEY\")\n",
    "if not voyage_api_key:\n",
    "    raise ValueError(\"VOYAGE_API_KEY not found in .env file or environment variables.\")\n",
    "vo = voyageai.Client(api_key=voyage_api_key)\n",
    "\n",
    "# Initialize MongoDB Client\n",
    "mongo_uri = os.environ.get(\"MONGO_URI\")\n",
    "if not mongo_uri:\n",
    "    raise ValueError(\"MONGO_URI not found in .env file or environment variables.\")\n",
    "client = MongoClient(mongo_uri)\n",
    "\n",
    "# Select your database and collection\n",
    "db = client['rag_db']\n",
    "collection = db['documents']\n",
    "\n",
    "print(\"Clients initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define User Query and Embed It with Voyage-AI\n",
    "\n",
    "We'll take a sample user question and convert it into a vector embedding. Remember to specify `input_type=\"query\"` when embedding search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Tell me about the upcoming features of the software.\"\n",
    "print(f\"\\nUser Query: {user_query}\")\n",
    "\n",
    "print(\"Generating query embedding with Voyage-AI...\")\n",
    "try:\n",
    "    query_embedding_response = vo.embed(\n",
    "        texts=[user_query],\n",
    "        model=\"voyage-large-2\", # Use the same model as for indexing\n",
    "        input_type=\"query\" \n",
    "    )\n",
    "    query_embedding = query_embedding_response.embeddings[0]\n",
    "    print(f\"Query embedding generated. Dimension: {len(query_embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating query embedding: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform Vector Search in MongoDB Atlas\n",
    "\n",
    "Now we'll use the `$vectorSearch` aggregation stage to find documents in our MongoDB collection that are semantically similar to our `user_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "  {\n",
    "    '$vectorSearch': {\n",
    "      'queryVector': query_embedding,\n",
    "      'path': 'embedding',          # The field where your embeddings are stored\n",
    "      'numCandidates': 50,          # Number of nearest neighbors to search\n",
    "      'limit': 3,                   # Number of top results to return\n",
    "      'index': 'default'            # Name of your Vector Search index (created in Lab 2)\n",
    "      # 'filter': { 'source': { '$eq': 'specific_document' } } # Optional: add filters\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    '$project': {\n",
    "      'text_chunk': 1,\n",
    "      'source': 1,\n",
    "      'score': { '$meta': 'vectorSearchScore' }, # Include similarity score\n",
    "      '_id': 0 # Exclude _id from results\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "print(\"Performing vector search in MongoDB Atlas...\")\n",
    "retrieved_documents = list(collection.aggregate(pipeline))\n",
    "\n",
    "if retrieved_documents:\n",
    "    print(f\"Retrieved {len(retrieved_documents)} relevant documents:\")\n",
    "    for doc in retrieved_documents:\n",
    "        print(f\"  - Score: {doc['score']:.4f}, Source: {doc['source']}, Text: {doc['text_chunk'][:70]}...\")\n",
    "else:\n",
    "    print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Context for the LLM\n",
    "\n",
    "We combine the `text_chunk` from the retrieved documents to form a single context string that will be passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([doc['text_chunk'] for doc in retrieved_documents])\n",
    "\n",
    "print(\"\\n--- Retrieved Context for LLM ---\")\n",
    "print(context)\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "if not context:\n",
    "    print(\"Warning: No context was retrieved. The LLM might not be able to answer accurately.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Construct Augmented Prompt for LLM\n",
    "\n",
    "Finally, we create the full prompt that will be sent to our LLM. This prompt includes instructions for the LLM, the retrieved context, and the original user question. For this lab, we will only construct the prompt, not make an actual LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if context:\n",
    "    llm_prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the user's question based on the provided context only.\n",
    "If you cannot find the answer in the context, politely state that the information is not available.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "else:\n",
    "    llm_prompt = f\"\"\"\n",
    "You are a helpful assistant. I couldn't find relevant information for the following question.\n",
    "Please state that the information is not available in the provided knowledge base.\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- LLM Augmented Prompt ---\")\n",
    "print(llm_prompt)\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# You would then make your LLM API call here, e.g.:\n",
    "# from openai import OpenAI\n",
    "# client_openai = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# completion = client_openai.chat.completions.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": llm_prompt}\n",
    "#     ]\n",
    "# )\n",
    "# llm_response = completion.choices[0].message.content\n",
    "# print(f\"\\nLLM Response: {llm_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully built the core retrieval and augmentation steps of a RAG pipeline! The next step in a full application would be to send this `llm_prompt` to an actual LLM and process its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to close the MongoDB client connection\n",
    "client.close()\n",
    "print(\"MongoDB client connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}